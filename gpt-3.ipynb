{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from langchain.prompts import Prompt, BaseChatPromptTemplate\n",
    "from langchain.schema import BaseOutputParser\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.chains import LLMChain\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "import pandas as pd\n",
    "import dotenv\n",
    "import random\n",
    "import networkx as nx\n",
    "from langchain.schema import (\n",
    "    BaseMessage, \n",
    "    HumanMessage, \n",
    "    SystemMessage,\n",
    "    AIMessage\n",
    ")\n",
    "dotenv.load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "pairs_df = pd.read_csv('data/acm_ccs_clean.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_nodes_edges(pairs_df, group):\n",
    "    pairs = pairs_df[pairs_df['group'] == group]\n",
    "    edges = []\n",
    "    nodes = set()\n",
    "    for i, row in pairs.iterrows():\n",
    "        parent, child = row['parent'], row['child']\n",
    "        parent = parent.replace('_', ' ')\n",
    "        child = child.replace('_', ' ')\n",
    "        edges.append({\n",
    "            'parent': parent,\n",
    "            'child': child,\n",
    "        })\n",
    "        nodes.add(parent)\n",
    "        nodes.add(child)\n",
    "\n",
    "    nodes = list(nodes)    \n",
    "    # randomly shffle the nodes and relations to avoid learning pattern\n",
    "    random.shuffle(edges)\n",
    "    random.shuffle(nodes)\n",
    "\n",
    "    return nodes, edges\n",
    "\n",
    "def get_groups(pairs_df, split='train'):\n",
    "    groups = pairs_df[pairs_df['type'] == split]['group'].unique()\n",
    "    return groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class TaxomomyPrompt(BaseChatPromptTemplate):\n",
    "    def format_messages(self, **kwargs) -> list[BaseMessage]:\n",
    "        group_examples = kwargs['group_examples']\n",
    "        concepts = kwargs['concepts']\n",
    "\n",
    "        prefix_prompt = (\n",
    "            \"You are an expert constructing a taxonomy from a list of concepts. Given a list of concepts,\"\n",
    "            \"construct a taxonomy by creating a list of their parent-child relationships.\\n\\n\"\n",
    "        )\n",
    "\n",
    "        prefix_message = SystemMessage(content=prefix_prompt)\n",
    "\n",
    "        example_messages = []\n",
    "        for nodes, edges in group_examples:\n",
    "            node_prompt = '; '.join(nodes)\n",
    "            edge_prompt = '; '.join([f\"{edge['child']} is a subtopic of {edge['parent']}\" for edge in edges])\n",
    "\n",
    "            example_messages.append(\n",
    "                HumanMessage(content=f\"Concepts: {node_prompt}\\nRelationships: \")\n",
    "            )\n",
    "\n",
    "            example_messages.append(\n",
    "                AIMessage(content=f\"{edge_prompt}\\n\\n\")\n",
    "            )\n",
    "        \n",
    "        concepts = '; '.join(concepts)\n",
    "        question_message = HumanMessage(content=f\"Concepts: {concepts}\\nRelationships: \")\n",
    "\n",
    "\n",
    "        return [prefix_message] + example_messages + [question_message]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "few_shot_numbers = 5\n",
    "\n",
    "train_groups = get_groups(pairs_df)\n",
    "test_groups = get_groups(pairs_df, split='test')\n",
    "\n",
    "def generate_train_examples(pairs_df, num_examples=5):\n",
    "    random.shuffle(train_groups)\n",
    "    train_group_examples = []\n",
    "    for group in train_groups[:num_examples]:\n",
    "        nodes, edges = get_nodes_edges(pairs_df, group)\n",
    "        train_group_examples.append((nodes, edges))\n",
    "    return train_group_examples\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TaxonomyParser(BaseOutputParser):\n",
    "    def parse(self, output):\n",
    "        output = output.split(';')\n",
    "        output = [line for line in output if line != '']\n",
    "        output = [re.split(' is a subtopic of ', line) for line in output]\n",
    "        print(output)\n",
    "        result = []\n",
    "        for line in output:\n",
    "            if len(line) != 2:\n",
    "                continue\n",
    "            result.append({\n",
    "                'child': line[0].strip(),\n",
    "                'parent': line[1].strip()\n",
    "            })\n",
    "        return result\n",
    "    \n",
    "def call_chain(chain, concepts, group_examples, num_retries=5):\n",
    "    count = 0\n",
    "    while count < num_retries:\n",
    "        try:\n",
    "            result = chain.run(\n",
    "                concepts=concepts,\n",
    "                group_examples=group_examples,\n",
    "            )\n",
    "            return result\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            count += 1\n",
    "    raise Exception(\"Failed to generate result\")\n",
    "\n",
    "prompt = TaxomomyPrompt(\n",
    "    input_variables=['concepts', 'group_examples'],\n",
    ")\n",
    "llm = ChatOpenAI(model='gpt-4')# ChatOpenAI(model='gpt-3.5-turbo')\n",
    "chain = LLMChain(llm=llm, prompt=prompt, output_parser=TaxonomyParser())\n",
    "results = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/15 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "System: You are an expert constructing a taxonomy from a list of concepts. Given a list of concepts,construct a taxonomy by creating a list of their parent-child relationships.\n",
      "\n",
      "\n",
      "Human: Concepts: Law; Sociology; Economics; Psychology; Law, social and behavioral sciences; Ethnography; Anthropology\n",
      "Relationships: \n",
      "AI: Law is a subtopic of Law, social and behavioral sciences; Ethnography is a subtopic of Anthropology; Psychology is a subtopic of Law, social and behavioral sciences; Sociology is a subtopic of Law, social and behavioral sciences; Anthropology is a subtopic of Law, social and behavioral sciences; Economics is a subtopic of Law, social and behavioral sciences\n",
      "\n",
      "\n",
      "Human: Concepts: Internet communications tools; Simple Object Access Protocol (SOAP); Service discovery and interfaces; Web crawling; Online advertising; Secure online transactions; Web data description languages; Social advertising; Social tagging; Deep web; Content match advertising; Personalization; Web Ontology Language (OWL); Web Services Description Language (WSDL); Wikis; Web services; Surfacing; Hypertext languages; Reputation systems; Markup languages; Online auctions; Crowdsourcing; Email; Web search engines; Chat; Electronic funds transfer; Semantic web description languages; Display advertising; Web conferencing; Online shopping; Web log analysis; Extensible Markup Language (XML); Trust; Web searching and information discovery; Web applications; Collaborative filtering; Electronic data interchange; RESTful web services; Mashups; Content ranking; Online banking; Browsers; Site wrapping; Social networks; Electronic commerce; Incentive schemes; Data extraction and integration; Spam detection; Page and site ranking; Texting; Web mining; E-commerce infrastructure; Web interfaces; Digital cash; World Wide Web; Sponsored search advertising; Web indexing; Search results deduplication; Resource Description Framework (RDF); Traffic analysis; Blogs; Social recommendation; Answer ranking; Universal Description Discovery and Integration (UDDI)\n",
      "Relationships: \n",
      "AI: Semantic web description languages is a subtopic of Web data description languages; Incentive schemes is a subtopic of Crowdsourcing; Chat is a subtopic of Internet communications tools; Reputation systems is a subtopic of Crowdsourcing; Electronic commerce is a subtopic of Web applications; Web indexing is a subtopic of Web search engines; Secure online transactions is a subtopic of Electronic commerce; Mashups is a subtopic of Web interfaces; Collaborative filtering is a subtopic of Web searching and information discovery; Electronic data interchange is a subtopic of Electronic commerce; Web crawling is a subtopic of Web search engines; Content ranking is a subtopic of Web searching and information discovery; Data extraction and integration is a subtopic of Web mining; Online auctions is a subtopic of Electronic commerce; Universal Description Discovery and Integration (UDDI) is a subtopic of Web services; Site wrapping is a subtopic of Web mining; Wikis is a subtopic of Web interfaces; Web searching and information discovery is a subtopic of World Wide Web; Web applications is a subtopic of World Wide Web; Internet communications tools is a subtopic of Web applications; Electronic funds transfer is a subtopic of Electronic commerce; Surfacing is a subtopic of Data extraction and integration; Web log analysis is a subtopic of Web mining; Social recommendation is a subtopic of Web searching and information discovery; Answer ranking is a subtopic of Crowdsourcing; Web conferencing is a subtopic of Internet communications tools; Online shopping is a subtopic of Electronic commerce; Display advertising is a subtopic of Online advertising; Extensible Markup Language (XML) is a subtopic of Markup languages; Online banking is a subtopic of Electronic commerce; Crowdsourcing is a subtopic of Web applications; Hypertext languages is a subtopic of Markup languages; Resource Description Framework (RDF) is a subtopic of Semantic web description languages; Sponsored search advertising is a subtopic of Online advertising; Blogs is a subtopic of Internet communications tools; Content match advertising is a subtopic of Online advertising; Browsers is a subtopic of Web interfaces; Personalization is a subtopic of Web searching and information discovery; Page and site ranking is a subtopic of Web search engines; E-commerce infrastructure is a subtopic of Electronic commerce; Web interfaces is a subtopic of World Wide Web; Social networks is a subtopic of Web applications; Web search engines is a subtopic of Web searching and information discovery; Digital cash is a subtopic of Electronic commerce; Email is a subtopic of Internet communications tools; Web Ontology Language (OWL) is a subtopic of Semantic web description languages; Deep web is a subtopic of Data extraction and integration; Texting is a subtopic of Internet communications tools; Web mining is a subtopic of World Wide Web; Traffic analysis is a subtopic of Web mining; Search results deduplication is a subtopic of Data extraction and integration; Markup languages is a subtopic of Web data description languages; Simple Object Access Protocol (SOAP) is a subtopic of Web services; Web data description languages is a subtopic of World Wide Web; Web Services Description Language (WSDL) is a subtopic of Web services; Social advertising is a subtopic of Online advertising; Spam detection is a subtopic of Web search engines; Online advertising is a subtopic of World Wide Web; Service discovery and interfaces is a subtopic of Web services; RESTful web services is a subtopic of Web services; Social tagging is a subtopic of Web searching and information discovery; Web services is a subtopic of World Wide Web; Trust is a subtopic of Crowdsourcing\n",
      "\n",
      "\n",
      "Human: Concepts: Supply chain management; Transportation; Multi-criterion optimization and decision-making; Industry and manufacturing; Marketing; Operations research; Computer-aided manufacturing; Command and control; Forecasting; Decision analysis; Consumer products\n",
      "Relationships: \n",
      "AI: Computer-aided manufacturing is a subtopic of Operations research; Transportation is a subtopic of Operations research; Consumer products is a subtopic of Operations research; Forecasting is a subtopic of Operations research; Industry and manufacturing is a subtopic of Operations research; Supply chain management is a subtopic of Industry and manufacturing; Decision analysis is a subtopic of Operations research; Command and control is a subtopic of Industry and manufacturing; Marketing is a subtopic of Operations research; Multi-criterion optimization and decision-making is a subtopic of Decision analysis\n",
      "\n",
      "\n",
      "Human: Concepts: Network performance analysis; Network performance modeling; Network experimentation; Network measurement; Network performance evaluation; Network simulations\n",
      "Relationships: \n",
      "AI: Network experimentation is a subtopic of Network performance evaluation; Network performance analysis is a subtopic of Network performance evaluation; Network simulations is a subtopic of Network performance evaluation; Network performance modeling is a subtopic of Network performance evaluation; Network measurement is a subtopic of Network performance evaluation\n",
      "\n",
      "\n",
      "Human: Concepts: Distributed algorithms; Self-organization; Distributed computing methodologies; MapReduce algorithms; Distributed programming languages\n",
      "Relationships: \n",
      "AI: Self-organization is a subtopic of Distributed algorithms; Distributed programming languages is a subtopic of Distributed computing methodologies; Distributed algorithms is a subtopic of Distributed computing methodologies; MapReduce algorithms is a subtopic of Distributed algorithms\n",
      "\n",
      "\n",
      "Human: Concepts: Density estimation; Statistical paradigms; Decision diagrams; Markov-chain Monte Carlo convergence measures; Quantile regression; Probabilistic inference problems; Dimensionality reduction; Hypothesis testing and confidence interval computation; Metropolis-Hastings algorithm; Bayesian computation; Simulated annealing; Statistical graphics; Contingency table analysis; Spline models; Bayesian networks; Exploratory data analysis; Distribution functions; Nonparametric representations; Gibbs sampling; Stochastic differential equations; Nonparametric statistics; Resampling methods; Computing most probable explanation; Probabilistic representations; Queueing theory; Markov processes; Renewal theory; Time series analysis; Cluster analysis; Variational methods; Maximum likelihood estimation; Jackknifing; Causal networks; Variable elimination; Probabilistic reasoning algorithms; Robust regression; Probability and statistics; Kernel density estimators; Markov networks; Probabilistic algorithms; Bootstrapping; Max marginal computation; Survival analysis; Expectation maximization; Multivariate statistics; Random number generation; Regression analysis; Sequential Monte Carlo methods; Bayesian nonparametric models; Markov-chain Monte Carlo methods; Loopy belief propagation; Kalman filters and hidden Markov models; Stochastic processes; Equational models; Factor graphs\n",
      "Relationships: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for test_group in tqdm(test_groups):\n",
    "    concepts, edges = get_nodes_edges(pairs_df, test_group)\n",
    "    group_examples = generate_train_examples(pairs_df, few_shot_numbers)\n",
    "\n",
    "    print(prompt.format(\n",
    "        concepts=concepts,\n",
    "        group_examples=group_examples,\n",
    "    ))\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/15 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1845\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 1/15 [00:24<05:36, 24.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['Probability and statistics is a parent topic for all the concepts'], [' Bayesian computation, Bayesian networks, Gibbs sampling, Markov networks, Markov processes, Markov-chain Monte Carlo convergence measures, Markov-chain Monte Carlo methods, Metropolis-Hastings algorithm are subtopics of Probabilistic reasoning algorithms'], [' Hypothesis testing and confidence interval computation, Maximum likelihood estimation, Regression analysis, Robust regression are subtopics of Statistical paradigms'], [' Resampling methods, Jackknifing, Bootstrapping are subtopics of Nonparametric statistics'], [' Density estimation, Kernel density estimators are subtopics of Nonparametric representations'], [' Multivariate statistics, Cluster analysis, Dimensionality reduction are subtopics of Exploratory data analysis'], [' Factor graphs, Decision diagrams are subtopics of Probabilistic representations'], [' Causal networks, Bayesian networks, Markov networks are subtopics of Equational models'], [' Contingency table analysis, Survival analysis, Queueing theory are subtopics of Distribution functions'], [' Kalman filters and hidden Markov models, Stochastic processes, Stochastic differential equations, Sequential Monte Carlo methods, Simulated annealing are subtopics of Stochastic algorithms'], [' Random number generation', 'Statistical graphics'], [' Variable elimination, Loopy belief propagation, Max marginal computation are subtopics of Probabilistic inference problems'], [' Time series analysis, Spline models, Quantile regression are subtopics of Regression analysis'], [' Expectation maximization, Bayesian nonparametric models are subtopics of Bayesian computation'], [' Computing most probable explanation', 'Probabilistic reasoning algorithms'], [' Variational methods', 'Probability and statistics'], [' Renewal theory', 'Stochastic processes.']]\n",
      "1330\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|█▎        | 2/15 [00:26<02:27, 11.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['Statistical software', 'Mathematical software'], [' Mathematical software performance', 'Mathematical software'], [' Solvers', 'Mathematical software.']]\n",
      "3003\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 3/15 [00:33<01:50,  9.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['Differential calculus', 'Calculus'], [' Integral calculus', 'Calculus'], [' Point-set topology', 'Topology'], [' Geometric topology', 'Topology'], [' Algebraic topology', 'Topology'], [' Calculus', 'Continuous mathematics'], [' Topology', 'Continuous mathematics'], [' Lambda calculus', 'Calculus'], [' Continuous functions', 'Continuous mathematics.']]\n",
      "1557\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 27%|██▋       | 4/15 [00:56<02:44, 14.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['Information retrieval is a parent topic of Document filtering, Presentation of retrieval results, Query representation, Video search, Novelty in information retrieval, Search interfaces, Distributed retrieval, Search engine architectures and scalability, Language models, Document collection models, Multilingual and cross-lingual retrieval, Music retrieval, Environment-specific retrieval, Business intelligence, Combination, fusion and federated search, Information extraction, Content analysis and feature selection, Probabilistic retrieval models, Enterprise search, Top-k retrieval in databases, Information retrieval query processing, Retrieval effectiveness, Near-duplicate and plagiarism detection, Summarization, Link and co-citation analysis, Search engine indexing, Similarity measures, Query reformulation, Question answering, Multimedia and multimodal retrieval, Evaluation of retrieval results, Searching with auxiliary databases, Structured text search, Ontologies, Chemical and biochemical retrieval, Test collections, Query suggestion, Mathematics retrieval, Desktop search, Information retrieval diversity, Retrieval on mobile devices, Web and social media search, Learning to rank, Task models, Document structure, Data encoding and canonicalization, Query intent, Sentiment analysis, Structure and multilingual text search, Peer-to-peer retrieval, Image search, Rank aggregation, Retrieval tasks and goals, Expert search, Thesauri, Document topic models, Relevance assessment, Recommender systems, Document representation, Dictionaries, Users and interactive retrieval, Search index compression, Collaborative search, Query log analysis, Specialized information retrieval, Retrieval efficiency, Speech / audio search, Personalization, Clustering and classification, Retrieval models and ranking, Adversarial retrieval.']]\n",
      "4334\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 5/15 [01:05<02:08, 12.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['Graphical / visual passwords', 'Authentication'], [' Access control', 'Security services'], [' Digital rights management', 'Security services'], [' Pseudonymity, anonymity and untraceability', 'Privacy-preserving protocols'], [' Multi-factor authentication', 'Authentication'], [' Privacy-preserving protocols', 'Security services'], [' Biometrics', 'Authentication'], [' Authorization', 'Access control'], [' Authentication', 'Security services\\n']]\n",
      "1647\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 6/15 [01:10<01:30, 10.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['Domain-specific security and privacy architectures', 'Software security engineering'], [' Software reverse engineering', 'Software security engineering'], [' Web application security', 'Software and application security'], [' Social network security and privacy', 'Software and application security'], [' Software and application security', 'Software security engineering']]\n",
      "2123\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 47%|████▋     | 7/15 [01:34<01:56, 14.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['Social recommendation', 'Social networks'], [' Collaborative and social computing design and evaluation methods', 'Collaborative and social computing'], [' Computer supported cooperative work', 'Collaborative and social computing'], [' Collaborative and social computing devices', 'Collaborative and social computing'], [' Social networks', 'Collaborative and social computing'], [' Wikis', 'Collaborative content creation'], [' Collaborative and social computing theory, concepts and paradigms', 'Collaborative and social computing'], [' Asynchronous editors', 'Collaborative content creation'], [' Collaborative content creation', 'Collaborative and social computing'], [' Open source software', 'Collaborative and social computing systems and tools'], [' Social content sharing', 'Social networks'], [' Empirical studies in collaborative and social computing', 'Collaborative and social computing'], [' Social tagging', 'Social networks'], [' Social networking sites', 'Social networks'], [' Ethnographic studies', 'Collaborative and social computing'], [' Reputation systems', 'Social networks'], [' Social navigation', 'Social networks'], [' Collaborative filtering', 'Social networks'], [' Social media', 'Social networks'], [' Blogs', 'Social content sharing'], [' Social tagging systems', 'Social networks'], [' Collaborative and social computing systems and tools', 'Collaborative and social computing'], [' Social network analysis', 'Social networks'], [' Synchronous editors', 'Collaborative content creation'], [' Social engineering (social sciences)', 'Collaborative and social computing.']]\n",
      "904\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 53%|█████▎    | 8/15 [02:08<02:24, 20.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['Technology and censorship', 'Computing / technology policy'], [' Broadband access', 'Internet governance / domain names'], [' Patents', 'Intellectual property'], [' Hardware reverse engineering', 'Antitrust and competition'], [' Universal access', 'Internet governance / domain names'], [' Acceptable use policy restrictions', 'Internet governance / domain names'], [' Import / export controls', 'Governmental regulations'], [' Identity theft', 'Computer crime'], [' Medical technologies', 'Medical information policy'], [' Financial crime', 'Computer crime'], [' Network access restrictions', 'Internet governance / domain names'], [' Network access control', 'Internet governance / domain names'], [' Censorship', 'Technology and censorship'], [' Digital rights management', 'Intellectual property'], [' Social engineering attacks', 'Computer crime'], [' Copyrights', 'Intellectual property'], [' Pornography', 'Age-based restrictions'], [' Licensing', 'Intellectual property'], [' Database protection laws', 'Intellectual property'], [' Computer crime', 'Computing / technology policy'], [' Genetic information', 'Medical information policy'], [' Censoring filters', 'Technology and censorship'], [' Health information exchanges', 'Medical information policy'], [' Personal health records', 'Patient privacy'], [' Commerce policy', 'Government technology policy'], [' Trademarks', 'Intellectual property'], [' Surveillance', 'Technology and censorship'], [' Governmental surveillance', 'Government technology policy'], [' Secondary liability', 'Intellectual property'], [' Medical records', 'Patient privacy'], [' Intellectual property', 'Computing / technology policy'], [' Political speech', 'Government technology policy'], [' Privacy policies', 'Consumer products policy'], [' Medical information policy', 'Healthcare policy'], [' Treaties', 'Governmental regulations'], [' Taxation', 'Governmental regulations'], [' Patient privacy', 'Healthcare policy'], [' Internet governance / domain names', 'Computing / technology policy'], [' Computing / technology policy is a parent topic'], [' Antitrust and competition', 'Governmental regulations'], [' Remote medicine', 'Medical technologies'], [' Age-based restrictions', 'Governmental regulations'], [' Soft intellectual property', 'Intellectual property'], [' Government technology policy', 'Computing / technology policy'], [' Net neutrality', 'Internet governance / domain names'], [' Governmental regulations', 'Government technology policy'], [' Online auctions policy', 'Commerce policy'], [' Malware / spyware crime', 'Computer crime'], [' Software reverse engineering', 'Antitrust and competition'], [' Corporate surveillance', 'Surveillance'], [' Consumer products policy', 'Commerce policy'], [' Transborder data flow', 'Internet governance / domain names.']]\n",
      "1437\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 9/15 [03:42<04:21, 43.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['Phonology / morphology', 'Natural language processing'], [' Game tree search', 'Search methodologies'], [' Scene understanding', 'Computer vision'], [' Video segmentation', 'Computer vision tasks'], [' Visual inspection', 'Computer vision tasks'], [' Vision for robotics', 'Robotic planning'], [' Motion capture', 'Computer vision tasks'], [' Philosophical/theoretical foundations of artificial intelligence', 'Artificial intelligence'], [' Theory of mind', 'Cognitive science'], [' Planning for deterministic actions', 'Planning and scheduling'], [' Planning under uncertainty', 'Planning and scheduling'], [' Planning and scheduling', 'Artificial intelligence'], [' Interest point and salient region detections', 'Computer vision tasks'], [' Object recognition', 'Computer vision tasks'], [' Cognitive robotics', 'Robotics'], [' Tracking', 'Computer vision tasks'], [' Mobile agents', 'Intelligent agents'], [' Image representations', 'Computer vision representations'], [' Randomized search', 'Search methodologies'], [' Speech recognition', 'Natural language processing'], [' Video summarization', 'Computer vision tasks'], [' Computer vision problems', 'Computer vision'], [' Intelligent agents', 'Artificial intelligence'], [' Discrete space search', 'Search methodologies'], [' Image and video acquisition', 'Computer vision tasks'], [' Probabilistic reasoning', 'Reasoning about belief and knowledge'], [' Multi-agent planning', 'Multi-agent systems'], [' Image segmentation', 'Computer vision tasks'], [' Discourse, dialogue and pragmatics', 'Natural language processing'], [' Heuristic function construction', 'Search methodologies'], [' Logic programming and answer set programming', 'Artificial intelligence'], [' Reconstruction', 'Computer vision tasks'], [' Planning with abstraction and generalization', 'Planning and scheduling'], [' Cognitive science', 'Artificial intelligence'], [' Computational control theory', 'Control methods'], [' Abstraction and micro-operators', 'Search methodologies'], [' Epipolar geometry', 'Computer vision tasks'], [' Search with partial observations', 'Search methodologies'], [' Nonmonotonic, default reasoning and belief revision', 'Reasoning about belief and knowledge'], [' Multi-agent systems', 'Artificial intelligence'], [' Natural language generation', 'Natural language processing'], [' Knowledge representation and reasoning', 'Artificial intelligence'], [' Computational photography', 'Computer vision'], [' Shape representations', 'Computer vision representations'], [' Shape inference', 'Computer vision tasks'], [' Artificial intelligence', 'Computer Science'], [' Appearance and texture representations', 'Computer vision representations'], [' 3D imaging', 'Computer vision tasks'], [' Continuous space search', 'Search methodologies'], [' Robotic planning', 'Robotics'], [' Semantic networks', 'Knowledge representation and reasoning'], [' Computer vision tasks', 'Computer vision'], [' Ontology engineering', 'Knowledge representation and reasoning'], [' Cooperation and coordination', 'Multi-agent systems'], [' Hierarchical representations', 'Computer vision representations'], [' Search methodologies', 'Artificial intelligence'], [' Reasoning about belief and knowledge', 'Artificial intelligence'], [' Language resources', 'Natural language processing'], [' Computer vision representations', 'Computer vision'], [' Information extraction', 'Natural language processing'], [' Visual content-based indexing and retrieval', 'Computer vision tasks'], [' Evolutionary robotics', 'Robotics'], [' Hyperspectral imaging', 'Computer vision tasks'], [' Object detection', 'Computer vision tasks'], [' Causal reasoning and diagnostics', 'Reasoning about belief and knowledge'], [' Activity recognition and understanding', 'Computer vision tasks'], [' Natural language processing', 'Artificial intelligence'], [' Motion path planning', 'Robotic planning'], [' Scene anomaly detection', 'Computer vision tasks'], [' Vagueness and fuzzy logic', 'Reasoning about belief and knowledge'], [' Biometrics', 'Computer vision tasks'], [' Object identification', 'Computer vision tasks'], [' Computer vision', 'Artificial intelligence'], [' Spatial and physical reasoning', 'Reasoning about belief and knowledge'], [' Control methods', 'Robotics'], [' Matching', 'Computer vision tasks'], [' Lexical semantics', 'Natural language processing'], [' Machine translation', 'Natural language processing'], [' Distributed artificial intelligence', 'Artificial intelligence'], [' Description logics', 'Knowledge representation and reasoning'], [' Camera calibration', 'Computer vision tasks'], [' Active vision', 'Computer vision tasks'], [' Temporal reasoning', 'Reasoning about belief and knowledge.']]\n",
      "1939\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 10/15 [04:03<03:04, 36.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['Modeling methodologies', 'Modeling and simulation'], [' Artificial life', 'Modeling and simulation'], [' Interactive simulation', 'Simulation types and techniques'], [' Simulation by animation', 'Simulation types and techniques'], [' Systems theory', 'Modeling and simulation'], [' Molecular simulation', 'Simulation types and techniques'], [' Discrete-event simulation', 'Simulation types and techniques'], [' Simulation languages', 'Simulation tools'], [' Real-time simulation', 'Simulation types and techniques'], [' Model development and analysis', 'Modeling methodologies'], [' Simulation support systems', 'Simulation tools'], [' Rare-event simulation', 'Simulation types and techniques'], [' Model verification and validation', 'Model development and analysis'], [' Scientific visualization', 'Visual analytics'], [' Agent / discrete models', 'Continuous models'], [' Quantum mechanic simulation', 'Simulation types and techniques'], [' Uncertainty quantification', 'Simulation theory'], [' Multiscale systems', 'Systems theory'], [' Network science', 'Systems theory'], [' Simulation tools', 'Simulation environments'], [' Simulation evaluation', 'Simulation theory'], [' Visual analytics', 'Modeling and simulation'], [' Simulation environments', 'Modeling and simulation'], [' Continuous models', 'Modeling methodologies'], [' Data assimilation', 'Simulation theory'], [' Simulation types and techniques', 'Modeling and simulation'], [' Simulation theory', 'Modeling and simulation'], [' Continuous simulation', 'Simulation types and techniques'], [' Distributed simulation', 'Simulation types and techniques'], [' Massively parallel and high-performance simulations', 'Simulation types and techniques\\n\\n']]\n",
      "3130\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 73%|███████▎  | 11/15 [04:22<02:05, 31.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['Genomics', 'Life and medical sciences'], [' Health care information systems', 'Health informatics'], [' Computational transcriptomics', 'Transcriptomics'], [' Computational biology', 'Life and medical sciences'], [' Molecular sequence analysis', 'Genomics'], [' Population genetics', 'Genetics'], [' Imaging', 'Health informatics'], [' Molecular evolution', 'Molecular sequence analysis'], [' Computational genomics', 'Genomics'], [' Proteomics', 'Life and medical sciences'], [' Sequencing and genotyping technologies', 'Genomics'], [' Biological networks', 'Systems biology'], [' Bioinformatics', 'Life and medical sciences'], [' Molecular structural biology', 'Life and medical sciences'], [' Metabolomics / metabonomics', 'Life and medical sciences'], [' Recognition of genes and regulatory elements', 'Genomics'], [' Transcriptomics', 'Life and medical sciences'], [' Computational proteomics', 'Proteomics'], [' Health informatics', 'Life and medical sciences'], [' Consumer health', 'Health informatics'], [' Systems biology', 'Life and medical sciences'], [' Genetics', 'Life and medical sciences\\n']]\n",
      "2138\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 12/15 [04:39<01:20, 26.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['Sensor devices and platforms', 'Sensors and actuators'], [' Electro-mechanical devices', 'Sensors and actuators'], [' Haptic devices', 'Sensors and actuators'], [' Sensor applications and deployments', 'Sensors and actuators'], [' Wireless integrated network sensors', 'Sensors and actuators'], [' Tactile and hand-based interfaces', 'Signal processing systems'], [' Touch screens', 'Signal processing systems'], [' External storage', 'Communication hardware, interfaces and storage'], [' Printers', 'Communication hardware, interfaces and storage'], [' Scanners', 'Communication hardware, interfaces and storage'], [' Buses and high-speed links', 'Networking hardware'], [' Wireless devices', 'Networking hardware'], [' Beamforming', 'Digital signal processing'], [' Noise reduction', 'Digital signal processing'], [' Digital signal processing', 'Signal processing systems'], [' Sound-based input / output', 'Signal processing systems'], [' Displays and imagers', 'Signal processing systems\\n']]\n",
      "1519\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 87%|████████▋ | 13/15 [04:52<00:45, 22.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['Design for debug', 'Hardware validation'], [' Post-manufacture validation and debug', 'Hardware validation'], [' Coverage metrics', 'Functional verification'], [' Bug fixing (hardware)', 'Bug detection, localization and diagnosis'], [' Assertion checking', 'Functional verification'], [' Bug detection, localization and diagnosis', 'Hardware validation'], [' Power and thermal analysis', 'Physical verification'], [' Timing analysis and sign-off', 'Physical verification'], [' Semi-formal verification', 'Functional verification'], [' Layout-versus-schematics', 'Physical verification'], [' Transaction-level verification', 'Functional verification'], [' Equivalence checking', 'Physical verification'], [' Theorem proving and SAT solving', 'Model checking'], [' Design rule checking', 'Physical verification'], [' Simulation and emulation', 'Hardware validation'], [' Model checking', 'Functional verification'], [' Functional verification', 'Hardware validation'], [' Physical verification', 'Hardware validation\\n']]\n",
      "1419\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 93%|█████████▎| 14/15 [05:03<00:19, 19.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['Hardware test', 'Online test and diagnostics'], [' Test data compression', 'Online test and diagnostics'], [' Testing with distributed and parallel systems', 'Online test and diagnostics'], [' Test-pattern generation and fault simulation', 'Online test and diagnostics'], [' Fault models and test metrics', 'Online test and diagnostics'], [' Hardware reliability screening', 'Online test and diagnostics'], [' Design for testability', 'Online test and diagnostics'], [' Analog, mixed-signal and radio frequency test', 'Hardware test'], [' Built-in self-test', 'Hardware test'], [' Board- and system-level test', 'Hardware test'], [' Memory test and repair', 'Hardware test'], [' Defect-based test', 'Hardware test\\n']]\n",
      "3130\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15/15 [05:26<00:00, 21.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['Carbon based electronics', 'Emerging technologies'], [' Circuit substrates', 'Emerging technologies'], [' Quantum dots and cellular automata', 'Emerging technologies'], [' Quantum communication and cryptography', 'Quantum technologies'], [' Quantum error correction and fault tolerance', 'Quantum technologies'], [' Single electron devices', 'Emerging technologies'], [' Flexible and printable circuits', 'Emerging technologies'], [' III-V compounds', 'Emerging technologies'], [' Memory and dense storage', 'Emerging technologies'], [' Quantum computation', 'Quantum technologies'], [' Biology-related information processing', 'Bio-embedded electronics'], [' Microelectromechanical systems', 'Electromechanical systems'], [' Nanoelectromechanical systems', 'Electromechanical systems'], [' Spintronics and magnetic technologies', 'Emerging technologies'], [' Tunneling devices', 'Emerging technologies'], [' Reversible logic', 'Emerging technologies'], [' Emerging architectures', 'Emerging technologies'], [' Emerging optical and photonic technologies', 'Emerging technologies'], [' Superconducting circuits', 'Emerging technologies'], [' Emerging simulation', 'Emerging tools and methodologies'], [' Cellular neural networks', 'Neural systems'], [' Emerging interfaces', 'Emerging technologies'], [' Analysis and design of emerging devices and systems', 'Emerging tools and methodologies'], [' Plasmonics', 'Emerging technologies'], [' Emerging languages and compilers', 'Emerging tools and methodologies'], [' Quantum technologies', 'Emerging technologies'], [' Bio-embedded electronics', 'Emerging technologies'], [' Electromechanical systems', 'Emerging technologies'], [' Neural systems', 'Emerging technologies'], [' Emerging tools and methodologies', 'Emerging technologies\\n\\n']]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for test_group in tqdm(test_groups):\n",
    "    concepts, edges = get_nodes_edges(pairs_df, test_group)\n",
    "    group_examples = generate_train_examples(pairs_df, few_shot_numbers)\n",
    "\n",
    "    print(llm.get_num_tokens_from_messages(prompt.format_messages(\n",
    "        concepts=concepts,\n",
    "        group_examples=group_examples,\n",
    "    )))\n",
    "    result = call_chain(chain, concepts, group_examples)\n",
    "    results.append({\n",
    "        'group': test_group,\n",
    "        'nodes': concepts,\n",
    "        'result': result,\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['10002950_2', '10002950_3', '10002950_5', '10002951_5',\n",
       "       '10002978_3', '10002978_9', '10003120_3', '10003456_2',\n",
       "       '10010147_3', '10010147_5', '10010405_4', '10010583_2',\n",
       "       '10010583_7', '10010583_8', '10010583_10'], dtype=object)"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_pairs = []\n",
    "for result in results:\n",
    "    for edge in result['result']:\n",
    "        result_pairs.append({\n",
    "            'group': result['group'],\n",
    "            'child': edge['child'],\n",
    "            'parent': edge['parent'],\n",
    "        })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(result_pairs)\n",
    "df.to_csv('./results/gpt-3/ccs/gpt4.csv', index=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import (\n",
    "    convert_to_ancestor_graph, \n",
    "    maximum_absorbance, \n",
    "    dataframe_to_ancestor_graph, \n",
    "    evaluate_groups,\n",
    "    maximum_likelihood,\n",
    "    maximum_branching,\n",
    "    majority_voting\n",
    ")\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "actual_tree='data/bansal_wordnet_true_pairs.csv' # bansal_wordnet_true_pairs acm_ccs_clean\n",
    "data_dir = f'./results/gpt-3/wordnet/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get individual best result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 114/114 [00:00<00:00, 1070.28it/s]\n"
     ]
    }
   ],
   "source": [
    "dfs = []\n",
    "num_generations = 5\n",
    "\n",
    "for i in range(1, num_generations + 1):\n",
    "    filename = f'{data_dir}/results_{i}.csv'\n",
    "    df = pd.read_csv(filename)\n",
    "    df['child'] = df['child'].apply(lambda x: x.replace(' ', '_'))\n",
    "    df['parent'] = df['parent'].apply(lambda x: x.replace(' ', '_'))\n",
    "    dfs.append(df)\n",
    "\n",
    "# get ground truth\n",
    "df_actual=pd.read_csv(actual_tree)\n",
    "df_actual=df_actual[df_actual['type'] == 'test']\n",
    "df_actual = dataframe_to_ancestor_graph(df_actual)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 114/114 [00:00<00:00, 932.42it/s]\n",
      "100%|██████████| 114/114 [00:00<00:00, 897.05it/s]\n",
      "100%|██████████| 114/114 [00:00<00:00, 801.07it/s]\n",
      "100%|██████████| 114/114 [00:00<00:00, 916.89it/s]\n",
      "100%|██████████| 114/114 [00:00<00:00, 1090.72it/s]\n",
      "100%|██████████| 114/114 [00:00<00:00, 786.25it/s]\n",
      "100%|██████████| 114/114 [00:00<00:00, 1028.07it/s]\n",
      "100%|██████████| 114/114 [00:00<00:00, 796.43it/s]\n",
      "100%|██████████| 114/114 [00:00<00:00, 968.85it/s]\n",
      "100%|██████████| 114/114 [00:00<00:00, 881.62it/s]\n"
     ]
    }
   ],
   "source": [
    "recalls = []\n",
    "precisions = []\n",
    "f1s = []\n",
    "\n",
    "for df in dfs:\n",
    "    df = dataframe_to_ancestor_graph(df)\n",
    "    recall, precision, f1 = evaluate_groups(df_actual, df)\n",
    "\n",
    "    recalls.append(recall)\n",
    "    precisions.append(precision)\n",
    "    f1s.append(f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Recall:  0.5364916307655639\n",
      "Best Precision:  0.646368967793948\n",
      "Best F1:  0.572489046346162\n"
     ]
    }
   ],
   "source": [
    "print(\"Best Recall: \", np.max(recalls))\n",
    "print(\"Best Precision: \", np.max(precisions))\n",
    "print(\"Best F1: \", np.max(f1s))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combine Multiple Predictions (Maximum branching)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = []\n",
    "num_generations = 5\n",
    "\n",
    "for i in range(1, num_generations + 1):\n",
    "    filename = f'{data_dir}/results_{i}.csv'\n",
    "    df = pd.read_csv(filename)\n",
    "    df['child'] = df['child'].apply(lambda x: x.replace(' ', '_'))\n",
    "    df['parent'] = df['parent'].apply(lambda x: x.replace(' ', '_'))\n",
    "    dfs.append(df)\n",
    "\n",
    "df = pd.concat(dfs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows = []\n",
    "columns = df.columns.tolist()\n",
    "\n",
    "for group_name, group_df in df.groupby(columns):\n",
    "    count = group_df.shape[0]\n",
    "    new_row = {\n",
    "        columns[0]: group_name[0],\n",
    "        columns[1]: group_name[1],\n",
    "        columns[2]: group_name[2],\n",
    "        'predict': count,\n",
    "    }\n",
    "    rows.append(new_row)\n",
    "\n",
    "merged_df = pd.DataFrame(rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 114/114 [00:00<00:00, 431.74it/s]\n"
     ]
    }
   ],
   "source": [
    "def convert_to_ancestor_graph(G):\n",
    "    '''Converts a (parent) tree to a graph with edges for all ancestor relations in the tree.'''\n",
    "    G_anc = nx.DiGraph()\n",
    "    for node in G.nodes():\n",
    "        for anc in nx.ancestors(G, node):\n",
    "            G_anc.add_edge(anc, node)\n",
    "    return G_anc\n",
    "\n",
    "import networkx as nx\n",
    "\n",
    "forest=[]\n",
    "for g in tqdm(list(set(merged_df.group))):\n",
    "    df = maximum_branching(merged_df, g)\n",
    "    forest.append(df)\n",
    "res_v2=pd.concat(forest, ignore_index=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_actual=pd.read_csv(actual_tree)\n",
    "df_actual=df_actual[df_actual['type'] == 'test']\n",
    "df_actual['child'] = df_actual['child'].apply(lambda x: x.replace(' ', '_'))\n",
    "df_actual['parent'] = df_actual['parent'].apply(lambda x: x.replace(' ', '_'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 114/114 [00:00<00:00, 1101.16it/s]\n"
     ]
    }
   ],
   "source": [
    "df_actual = dataframe_to_ancestor_graph(df_actual)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 114/114 [00:00<00:00, 822.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall:  0.6645223913488745 Precision:  0.6276596550517497 F1:  0.630155470051838\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "recall, precision, f1 = evaluate_groups(df_actual, res_v2)\n",
    "print(\"Recall: \", recall, \"Precision: \", precision, \"F1: \", f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>parent</th>\n",
       "      <th>child</th>\n",
       "      <th>group</th>\n",
       "      <th>compare</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [parent, child, group, compare]\n",
       "Index: []"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_actual[df_actual.group=='10010147_3']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>parent</th>\n",
       "      <th>child</th>\n",
       "      <th>group</th>\n",
       "      <th>compare</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [parent, child, group, compare]\n",
       "Index: []"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res_v2[res_v2.group=='10010147_3']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 114/114 [00:00<00:00, 494.88it/s]\n"
     ]
    }
   ],
   "source": [
    "recall = []\n",
    "precision = []\n",
    "f1 = []\n",
    "for group in tqdm(list(set(df_actual.group))):\n",
    "    group_actual = df_actual[df_actual.group == group]\n",
    "    group_pred = res_v2[res_v2.group == group]\n",
    "    recall.append(len(group_actual.merge(group_pred, on='compare')) / len(group_actual))\n",
    "    precision.append(len(group_actual.merge(group_pred, on='compare')) / len(group_pred))\n",
    "    if precision[-1] + recall[-1] == 0:\n",
    "        f1.append(0)\n",
    "    else:\n",
    "        f1.append(2 * (precision[-1] * recall[-1]) / (precision[-1] + recall[-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.48965134192568305"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.mean(recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7506532177798615"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(precision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5704412106495687"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
